{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCa2--VQoXTb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from google.cloud import aiplatform, storage\n",
        "\n",
        "# Vertex AI Initialization\n",
        "PROJECT_ID = \"your-project-id\"  # Replace with your GCP Project ID\n",
        "REGION = \"us-central1\"  # Use Matching Engine's supported region\n",
        "BUCKET_NAME = \"your-gcs-bucket-name\"  # Replace with your GCS bucket name\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gcloud auth login\n",
        "gcloud auth application-default login\n"
      ],
      "metadata": {
        "id": "ekDqUm1_pi4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step - 1: Define Schema"
      ],
      "metadata": {
        "id": "qf9NaXeZoeyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_schema_based_data(data, schema, vector_dim=384):\n",
        "    \"\"\"\n",
        "    Converts data into Vertex AI-compatible JSONL format based on the schema.\n",
        "\n",
        "    Args:\n",
        "        data (list): List of raw data dictionaries.\n",
        "        schema (dict): Schema definition for the collection.\n",
        "        vector_dim (int): Expected dimension of embedding vectors.\n",
        "\n",
        "    Returns:\n",
        "        list: List of JSON objects formatted for Vertex AI.\n",
        "    \"\"\"\n",
        "    jsonl_data = []\n",
        "    for item in data:\n",
        "        jsonl_entry = {\"id\": str(item[\"id\"]), \"embedding\": item[\"vector\"]}\n",
        "\n",
        "        # Add metadata fields from schema\n",
        "        metadata = {}\n",
        "        for field in schema[\"collections\"][0][\"fields\"]:\n",
        "            if field[\"name\"] not in [\"id\", \"vector\"]:\n",
        "                metadata[field[\"name\"]] = item.get(field[\"name\"])\n",
        "        jsonl_entry[\"metadata\"] = metadata\n",
        "\n",
        "        # Validate vector dimensions\n",
        "        if len(item[\"vector\"]) != vector_dim:\n",
        "            raise ValueError(f\"Vector dimension mismatch for id {item['id']}. Expected {vector_dim}, got {len(item['vector'])}\")\n",
        "\n",
        "        jsonl_data.append(jsonl_entry)\n",
        "    return jsonl_data\n",
        "\n",
        "def save_jsonl_to_gcs(data, local_file, bucket_name, destination_blob_name):\n",
        "    \"\"\"\n",
        "    Saves data as JSONL and uploads it to GCS.\n",
        "\n",
        "    Args:\n",
        "        data (list): Data to save.\n",
        "        local_file (str): Local path to save JSONL.\n",
        "        bucket_name (str): GCS bucket name.\n",
        "        destination_blob_name (str): Destination path in GCS.\n",
        "    \"\"\"\n",
        "    # Save to JSONL\n",
        "    with open(local_file, \"w\") as f:\n",
        "        for entry in data:\n",
        "            f.write(json.dumps(entry) + \"\\n\")\n",
        "    print(f\"Data saved locally to {local_file}\")\n",
        "\n",
        "    # Upload to GCS\n",
        "    client = storage.Client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "    blob.upload_from_filename(local_file)\n",
        "    print(f\"Uploaded {local_file} to gs://{bucket_name}/{destination_blob_name}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "hTWd2yLeoe_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Create and Deploy Matching Engine Index"
      ],
      "metadata": {
        "id": "9p-VrCqVopzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_and_deploy_index(dimensions, index_name=\"SEARCH_PDP_INDEX\", endpoint_name=\"SEARCH_PDP_ENDPOINT\"):\n",
        "    \"\"\"\n",
        "    Creates and deploys a Vertex AI Matching Engine Index.\n",
        "\n",
        "    Args:\n",
        "        dimensions (int): Dimension of the embeddings.\n",
        "        index_name (str): Name of the index.\n",
        "        endpoint_name (str): Name of the endpoint.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Resource names of the index and endpoint.\n",
        "    \"\"\"\n",
        "    # Create Index\n",
        "    index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
        "        display_name=index_name,\n",
        "        dimensions=dimensions,\n",
        "        approximate_neighbors_count=100,\n",
        "        distance_measure_type=\"DOT_PRODUCT\",\n",
        "        sync=True,\n",
        "    )\n",
        "    print(f\"Created Matching Engine Index: {index.resource_name}\")\n",
        "\n",
        "    # Deploy Index to Endpoint\n",
        "    endpoint = aiplatform.MatchingEngineIndexEndpoint.create(display_name=endpoint_name, sync=True)\n",
        "    endpoint.deploy_index(index=index.resource_name, sync=True)\n",
        "    print(f\"Deployed Index at Endpoint: {endpoint.resource_name}\")\n",
        "\n",
        "    return index.resource_name, endpoint.resource_name"
      ],
      "metadata": {
        "id": "Gkn6QcqJo1Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Query Matching Engine Index"
      ],
      "metadata": {
        "id": "TPDaaqn-o6on"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_index(endpoint_resource_name, query_vector, top_k=5):\n",
        "    \"\"\"\n",
        "    Queries the Matching Engine index for the most similar vectors.\n",
        "\n",
        "    Args:\n",
        "        endpoint_resource_name (str): The endpoint resource name.\n",
        "        query_vector (list): Query embedding vector.\n",
        "        top_k (int): Number of nearest neighbors to return.\n",
        "\n",
        "    Returns:\n",
        "        list: Query results with IDs and similarity scores.\n",
        "    \"\"\"\n",
        "    endpoint = aiplatform.MatchingEngineIndexEndpoint(endpoint_resource_name)\n",
        "\n",
        "    results = endpoint.match(\n",
        "        deployed_index_id=\"SEARCH_PDP_DEPLOYED_INDEX\",  # Replace with actual deployed index ID\n",
        "        queries=[query_vector],\n",
        "        num_neighbors=top_k,\n",
        "    )\n",
        "\n",
        "    # Parse Results\n",
        "    return [{\"id\": r[\"id\"], \"distance\": r[\"distance\"]} for r in results[0]]"
      ],
      "metadata": {
        "id": "nn4M1tZFo3gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Main Workflow"
      ],
      "metadata": {
        "id": "akGf6Ro4rEu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    # Replace with actual data\n",
        "    raw_data = [\n",
        "        {\"id\": 1, \"vector\": [0.1, 0.2, 0.3, ...], \"title\": \"Title 1\", \"description\": \"Description 1\", \"rank\": 5},\n",
        "        {\"id\": 2, \"vector\": [0.4, 0.5, 0.6, ...], \"title\": \"Title 2\", \"description\": \"Description 2\", \"rank\": 3},\n",
        "    ]\n",
        "\n",
        "    # Load schema\n",
        "    with open(\"schema 2.json\", \"r\") as schema_file:\n",
        "        schema = json.load(schema_file)\n",
        "\n",
        "    # Prepare data\n",
        "    print(\"Preparing data...\")\n",
        "    formatted_data = prepare_schema_based_data(raw_data, schema)\n",
        "\n",
        "    # Save to JSONL and upload to GCS\n",
        "    save_jsonl_to_gcs(formatted_data, \"data.jsonl\", BUCKET_NAME, \"data/SEARCH_PDP_DATA.jsonl\")\n",
        "\n",
        "    # Create and deploy index\n",
        "    print(\"Creating and deploying index...\")\n",
        "    index_name, endpoint_name = create_and_deploy_index(dimensions=384)\n",
        "\n",
        "    # Query the index\n",
        "    query_vector = [0.1, 0.2, 0.3, ...]  # Replace with actual query vector\n",
        "    print(\"Querying the index...\")\n",
        "    results = query_index(endpoint_name, query_vector)\n",
        "    print(\"Search Results:\", results)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "L-aF_v4ho9d3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}